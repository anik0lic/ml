{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import nltk\n",
        "import re\n",
        "import math\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from string import punctuation\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "%cd /content/drive/My Drive/masinsko/\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2ZroO6gmHDl",
        "outputId": "690bc632-b0ae-4022-a62d-6096c2638dc6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/My Drive/masinsko\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultinomialNaiveBayes:\n",
        "  def __init__(self, nb_classes, nb_words, pseudocount):\n",
        "    self.nb_classes = nb_classes\n",
        "    self.nb_words = nb_words\n",
        "    self.pseudocount = pseudocount\n",
        "\n",
        "  def fit(self, X, Y):\n",
        "    nb_examples = X.shape[0]\n",
        "\n",
        "    self.priors = np.bincount(Y) / nb_examples\n",
        "    print('Priors:')\n",
        "    print(self.priors)\n",
        "\n",
        "    # Racunamo broj pojavljivanja svake reci u svakoj klasi\n",
        "    occs = np.zeros((self.nb_classes, self.nb_words))\n",
        "    for i in range(nb_examples):\n",
        "      c = Y[i]\n",
        "      for w in range(self.nb_words):\n",
        "        cnt = X[i][w]\n",
        "        occs[c][w] += cnt\n",
        "    print('Occurences:')\n",
        "    print(occs)\n",
        "\n",
        "    # Racunamo P(Rec_i|Klasa) - likelihoods\n",
        "    self.like = np.zeros((self.nb_classes, self.nb_words))\n",
        "    for c in range(self.nb_classes):\n",
        "      for w in range(self.nb_words):\n",
        "        up = occs[c][w] + self.pseudocount\n",
        "        down = np.sum(occs[c]) + self.nb_words*self.pseudocount\n",
        "        self.like[c][w] = up / down\n",
        "    print('Likelihoods:')\n",
        "    print(self.like)\n",
        "\n",
        "  def predict(self, bow):\n",
        "    # Racunamo P(Klasa|bow) za svaku klasu\n",
        "    probs = np.zeros(self.nb_classes)\n",
        "    for c in range(self.nb_classes):\n",
        "      prob = np.log(self.priors[c])\n",
        "      for w in range(self.nb_words):\n",
        "        cnt = bow[w]\n",
        "        prob += cnt * np.log(self.like[c][w])\n",
        "      probs[c] = prob\n",
        "    prediction = np.argmax(probs)\n",
        "    return prediction\n",
        "\n",
        "  def predict_multiply(self, bow):\n",
        "    # Racunamo P(Klasa|bow) za svaku klasu\n",
        "    # Mnozimo i stepenujemo kako bismo uporedili rezultate sa slajdovima\n",
        "    probs = np.zeros(self.nb_classes)\n",
        "    for c in range(self.nb_classes):\n",
        "      prob = self.priors[c]\n",
        "      for w in range(self.nb_words):\n",
        "        cnt = bow[w]\n",
        "        prob *= self.like[c][w] ** cnt\n",
        "      probs[c] = prob\n",
        "    prediction = np.argmax(probs)\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "rbpewSXNslon"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"disaster-tweets.csv\")\n",
        "\n",
        "def clean(corpus):\n",
        "    clean_corpus = []\n",
        "    for doc in corpus:\n",
        "        if doc.strip():\n",
        "            words = wordpunct_tokenize(doc)\n",
        "            words_lower = [w.lower() for w in words]\n",
        "            words_filtered = [w for w in words_lower if w not in stop_punc]\n",
        "            words_cleaned = [re.sub(r'http\\S+', '', w) for w in words_filtered]\n",
        "            words_cleaned = [re.sub(r'[^\\w\\s]', '', w) for w in words_cleaned if w != '' and 'http' not in w and 'co' not in w]\n",
        "            words_cleaned = [re.sub(r'[^a-z]', '', w) for w in words_cleaned]\n",
        "            words_stemmed = [porter.stem(w) for w in words_cleaned]\n",
        "            if words_stemmed:\n",
        "                clean_corpus.append(words_stemmed)\n",
        "    return clean_corpus\n",
        "\n",
        "corpus = data['text']\n",
        "porter = PorterStemmer()\n",
        "\n",
        "stop_punc = set(stopwords.words('english')).union(set(punctuation))\n",
        "clean_corpus = clean(corpus)\n",
        "\n",
        "vocab_set = set()\n",
        "for doc in clean_corpus:\n",
        "  for word in doc:\n",
        "    vocab_set.add(word)\n",
        "vocab = list(vocab_set)\n",
        "\n",
        "np.set_printoptions(precision=2, linewidth=200)\n",
        "\n",
        "def occ_score(word, doc):\n",
        "   return 1 if word in doc else 0\n",
        "\n",
        "def numocc_score(word, doc):\n",
        "  return doc.count(word)\n",
        "\n",
        "def freq_score(word, doc):\n",
        "  return doc.count(word) / len(doc)\n",
        "\n",
        "word_freq = {}\n",
        "for doc in clean_corpus:\n",
        "    for word in doc:\n",
        "        if word in word_freq:\n",
        "            word_freq[word] += 1\n",
        "        else:\n",
        "            word_freq[word] = 1\n",
        "\n",
        "# uzimanje 10000 najcescih reci\n",
        "sorted_words = sorted(word_freq, key=word_freq.get, reverse=True)\n",
        "top_words = sorted_words[:10000]\n",
        "vocab_filtered = {word: idx for idx, word in enumerate(top_words)}\n",
        "\n",
        "for score_fn in [occ_score, numocc_score, freq_score]:\n",
        "    X = np.zeros((len(clean_corpus), len(vocab_filtered)), dtype=np.float32)\n",
        "    for doc_idx in range(len(clean_corpus)):\n",
        "        doc = clean_corpus[doc_idx]\n",
        "        for word in doc:\n",
        "            if word in vocab_filtered:\n",
        "                word_idx = vocab_filtered[word]\n",
        "                cnt = score_fn(word, doc)\n",
        "                X[doc_idx][word_idx] = cnt\n",
        "\n",
        "x_train, x_test, y_train_s, y_test_s = train_test_split(X, data['target'], test_size=0.2)\n",
        "y_train = [int(label) for label in y_train_s]\n",
        "y_test = [int(l) for l in y_test_s]\n",
        "\n",
        "model = MultinomialNaiveBayes(nb_classes=2, nb_words=1000, pseudocount=1)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "predictions = []\n",
        "for sample in x_test:\n",
        "    prediction = model.predict(sample)\n",
        "    predictions.append(prediction)\n",
        "\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# 4b\n",
        "positive_corpus = [clean_corpus[i] for i in range(len(clean_corpus)) if data['target'][i] == 0]\n",
        "negative_corpus = [clean_corpus[i] for i in range(len(clean_corpus)) if data['target'][i] == 1]\n",
        "\n",
        "positive_word_freq = {}\n",
        "negative_word_freq = {}\n",
        "for doc in positive_corpus:\n",
        "    for word in doc:\n",
        "        if word in positive_word_freq:\n",
        "            positive_word_freq[word] += 1\n",
        "        else:\n",
        "            positive_word_freq[word] = 1\n",
        "\n",
        "for doc in negative_corpus:\n",
        "    for word in doc:\n",
        "        if word in negative_word_freq:\n",
        "            negative_word_freq[word] += 1\n",
        "        else:\n",
        "            negative_word_freq[word] = 1\n",
        "\n",
        "sorted_positive_words = sorted(positive_word_freq, key=positive_word_freq.get, reverse=True)\n",
        "sorted_negative_words = sorted(negative_word_freq, key=negative_word_freq.get, reverse=True)\n",
        "\n",
        "top_positive_words = [word for word in sorted_positive_words[:6] if word.strip()]\n",
        "top_negative_words = [word for word in sorted_negative_words[:6] if word.strip()]\n",
        "\n",
        "print(\"Top 5 words in positive tweets:\", top_positive_words)\n",
        "print(\"Top 5 words in negative tweets:\", top_negative_words)\n",
        "\n",
        "LR_scores = {}\n",
        "for word in set(sorted_positive_words).intersection(set(sorted_negative_words)):\n",
        "    if positive_word_freq[word] >= 10 and negative_word_freq[word] >= 10:\n",
        "        LR_score = positive_word_freq[word] / negative_word_freq[word]\n",
        "        LR_scores[word] = LR_score\n",
        "\n",
        "sorted_LR_scores = sorted(LR_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "top_LR_words = sorted_LR_scores[:5]\n",
        "bottom_LR_words = sorted_LR_scores[-5:]\n",
        "\n",
        "print(\"Top 5 words with highest LR scores:\", top_LR_words)\n",
        "print(\"Top 5 words with lowest LR scores:\", bottom_LR_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrV3o6kyns1g",
        "outputId": "e3ae0666-e332-4a17-ffdc-f23bab40b9f9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Priors:\n",
            "[0.57 0.43]\n",
            "Occurences:\n",
            "[[3.92e+02 2.45e+01 8.45e+00 ... 5.85e-01 6.25e-02 7.34e-01]\n",
            " [3.65e+02 8.15e+00 2.07e+01 ... 4.33e-01 9.07e-01 2.61e-01]]\n",
            "Likelihoods:\n",
            "[[0.12 0.01 0.   ... 0.   0.   0.  ]\n",
            " [0.13 0.   0.01 ... 0.   0.   0.  ]]\n",
            "Accuracy: 0.6946815495732108\n",
            "Top 5 words in positive tweets: ['like', 'get', 'amp', 'new', 'go']\n",
            "Top 5 words in negative tweets: ['fire', 'bomb', 'kill', 'news', 'amp']\n",
            "Top 5 words with highest LR scores: [('full', 8.4), ('love', 6.944444444444445), ('obliter', 6.583333333333333), ('scream', 6.25), ('let', 5.625)]\n",
            "Top 5 words with lowest LR scores: [('pm', 0.19767441860465115), ('report', 0.19117647058823528), ('train', 0.1743119266055046), ('warn', 0.16393442622950818), ('kill', 0.11801242236024845)]\n"
          ]
        }
      ]
    }
  ]
}